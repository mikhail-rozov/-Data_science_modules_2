{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e74091-bbe9-4177-908f-8e2dee9e6000",
   "metadata": {},
   "source": [
    "### Домашняя работа по теме \"Введение в задачу классификации. Постановка задачи и подготовка данных.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23358f74-f7ff-4981-86bb-bd27d8c11c1e",
   "metadata": {},
   "source": [
    "##### 1. Приведите по 2 примера, когда лучше максимизировать Precision, а когда Recall.\n",
    "\n",
    "<u>Максимизация Precision:</u>\n",
    "\n",
    "a) Модель-советник для торговли на фондовом рынке. Классифицирует данные по котировкам ценных бумаг.\n",
    "Целевой класс - \"покупать\". Тут нам важна уверенность в принятом решении, чтобы не потерять деньги,\n",
    "даже если некоторые потенциально прибыльные сигналы при этом мы можем пропустить.\n",
    "\n",
    "б) Производственная линия на химическом заводе. По составу продукта модель определяет, соответствует ли он\n",
    "характеристикам для продажи конечному потребителю, либо для перевода на следующий этап производства.\n",
    "Целевой класс - \"соответствует\". Важна точность предсказания, т.к. существуют финансовые риски. Не так\n",
    "страшно забраковать хороший продукт, как выкатить на продажу плохой.  \n",
    "Кстати, если мы целевым классом выберем \"не соответствует\", то на первый план выходит Recall.\n",
    "Поправьте, пожалуйста, если не прав.\n",
    "\n",
    "<u>Максимизация Recall:</u>\n",
    "\n",
    "а) Модель, которая классифицирует отзывы в Интернете на плохие/хорошие. Целевой класс - \"плохой отзыв\".\n",
    "Здесь важно найти как можно больше плохих отзывов, несмотря на то, что в этот класс попадёт какое-то количество\n",
    "хороших отзывов.\n",
    "\n",
    "б) Радиотелескоп, который фиксирует сигналы различных параметров из космоса. По этим параметрам модель определяет,\n",
    "является ли сигнал интересным для учёных или это просто \"шум\". Целевой класс - \"сигнал интересен\". Важно цепляться\n",
    "за каждый потенциально интересный сигнал, даже если в результате дальнейшего изучения он таковым не окажется."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793693c-d8ea-444a-8181-43a8b2b7542d",
   "metadata": {},
   "source": [
    "##### 2. Почему мы используем F-меру, почему, например, нельзя просто взять среднее от Precision и Recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a40f9d-e584-4937-a4c6-dfb6d2baeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad82dbb-1403-43fd-a015-a28d7887f454",
   "metadata": {},
   "source": [
    "Возьмём пример из задачи № 1, где у нас торговый советник.  \n",
    "Составим массив pred, содержащий сигналы советника, где 1 - сигнал на покупку, 0 - отсутствие сигнала.  \n",
    "Также возьмём массив true, в котором 1 - нужно было покупать, 0 - ничего не надо было делать.  \n",
    "F-меру будем рассматривать на примере F1_score ($\\beta$ = 1), так как в данном случае это не принципиально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00317103-0025-4779-aef9-2a08bc433727",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0]\n",
    "pred = [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faed0ee0-5245-4e80-af85-31789426da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Напишем функцию для исключения дублирования кода\n",
    "def evaluate(true, pred):\n",
    "    print('Precision:')\n",
    "    print(f'{precision_score(true, pred)}\\n')\n",
    "    print('Recall:')\n",
    "    print(f'{recall_score(true, pred)}\\n')\n",
    "    print('F1_score:')\n",
    "    print(f'{f1_score(true, pred)}\\n')\n",
    "    print('Среднее между Precision и Recall:')\n",
    "    print(f'{(precision_score(true, pred) + recall_score(true, pred)) / 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1de75-c0ee-4c83-ae79-0301e31271b7",
   "metadata": {},
   "source": [
    "Рассчитаем метрики этой модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4f3514-e363-4b1a-be39-2a8afd94ae01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      "0.8\n",
      "\n",
      "Recall:\n",
      "0.6666666666666666\n",
      "\n",
      "F1_score:\n",
      "0.7272727272727272\n",
      "\n",
      "Среднее между Precision и Recall:\n",
      "0.7333333333333334\n"
     ]
    }
   ],
   "source": [
    "evaluate(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093fe06a-71fd-4815-873a-e270eaedb138",
   "metadata": {},
   "source": [
    "Теперь представим, что наша модель во всех ситуациях советует нам покупать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb7fdd8-b1ef-430c-89cd-3329a800377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb1643-7a17-4d60-9711-d262d36d89a5",
   "metadata": {},
   "source": [
    "Теперь метрики выглядят следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f846530-f32b-4fef-a551-231f9cb99db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      "0.4\n",
      "\n",
      "Recall:\n",
      "1.0\n",
      "\n",
      "F1_score:\n",
      "0.5714285714285715\n",
      "\n",
      "Среднее между Precision и Recall:\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "evaluate(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65a24f-657a-4fde-97b5-4c58c8eb1637",
   "metadata": {},
   "source": [
    "А теперь наоборот - пусть модель практически никогда не даёт нам сигналов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe58a416-7170-41c4-bca1-da0aac902462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92ffc89-f77e-441c-8725-d18c0c086954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      "1.0\n",
      "\n",
      "Recall:\n",
      "0.16666666666666666\n",
      "\n",
      "F1_score:\n",
      "0.2857142857142857\n",
      "\n",
      "Среднее между Precision и Recall:\n",
      "0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "evaluate(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c16cc4-8e1d-4b8f-9315-e7413037d2a0",
   "metadata": {},
   "source": [
    "Первоначально метрика f1_score и средняя между precision и recall были примерно равны.  \n",
    "Когда мы изменили предсказания и модель начала всегда советовать нам покупать, средняя двух метрик упала незначительно, а F1_score упала сильнее - с 0.72 до 0.57, - указывая на то, что качество предсказаний модели существенно снизилось (так и есть, хоть мы и получили максимальный recall, точность оставляет желать лучшего).  \n",
    "Когда модель дала всего одно предсказание, мы получили максимальную точность и очень маленький recall. При этом средняя двух метрик упала всего лишь до 0.58, а f1_score - до 0.28.  \n",
    "  \n",
    "<u>Вывод:</u> В сравнении со средней величиной между Precision и Recall, метрика f1_score существенно снижается, когда падает хотя бы что-то одно: точность или полнота. Это логично, учитывая, что в числителе этой метрики произведение двух метрик, тогда как среднее имеет дело только с суммой. То есть, среднему не важно, если у нас одна из метрик сильно упала, лишь бы другая компенсировала это падение. Но реальные модели должны быть хороши и в точности, и в полноте, и f1_score отслеживает эту комбинацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b27070e-08f6-460e-bb06-a507372b1659",
   "metadata": {},
   "source": [
    "##### 3. *Реализовать функции для подсчета Accuracy, Precision, Recall, F-score, которые на вход принимают y_true (истинные значения), y_pred (предсказанные значения), а на выход дается метрика."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20315f72-0e36-425e-b1d8-5fe48e07606c",
   "metadata": {},
   "source": [
    "Так как по условию задачи функции должны принимать на вход только y_true и y_pred, то F-score будем считать с $\\beta$ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa6c755-e282-4df9-8f4a-3a749c56799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a788ab35-7d33-46da-aedc-d233cf0ed532",
   "metadata": {},
   "source": [
    "Зададим явно значения y_true и y_pred для возможности проверки работы функций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30af3753-e7f3-41b1-8bf4-060a92e063a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0])\n",
    "y_pred = np.array([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d45699-f454-468f-9f88-2f3762ee435f",
   "metadata": {},
   "source": [
    "Создадим класс с функциями для подсчёта метрик и сравнения их со встроенными функциями подсчёта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e59c058-ef34-47aa-a895-c4417e639ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMetrics:\n",
    "    \n",
    "    def __init__(self, y_true, y_pred):\n",
    "        \n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "        # True Negative\n",
    "        self.TN = ((self.y_true == 0) & (self.y_pred == 0)).sum()\n",
    "        \n",
    "        # False Positive\n",
    "        self.FP = ((self.y_true == 0) & (self.y_pred == 1)).sum()\n",
    "        \n",
    "        # False Negative\n",
    "        self.FN = ((self.y_true == 1) & (self.y_pred == 0)).sum()\n",
    "        \n",
    "        # True Positive\n",
    "        self.TP = ((self.y_true == 1) & (self.y_pred == 1)).sum()\n",
    "    \n",
    "    def my_accuracy(self):\n",
    "        return (self.TP + self.TN) / (self.TP + self.FP + self.TN + self.FN)\n",
    "    \n",
    "    def my_precision(self):\n",
    "        return self.TP / (self.TP + self.FP)\n",
    "    \n",
    "    def my_recall(self):\n",
    "        return self.TP / (self.TP + self.FN)\n",
    "    \n",
    "    def my_fscore(self):\n",
    "        return 2 * self.my_precision() * self.my_recall() / (self.my_precision() + self.my_recall())\n",
    "    \n",
    "    def compare(self):\n",
    "        \n",
    "        print(f'My accuracy score:\\t\\t{self.my_accuracy()}\\nBuilt-in accuracy score:\\t{accuracy_score(self.y_true, self.y_pred)}\\n')\n",
    "        \n",
    "        print(f'My precision score:\\t\\t{self.my_precision()}\\nBuilt-in precision score:\\t{precision_score(self.y_true, self.y_pred)}\\n')\n",
    "        \n",
    "        print(f'My recall score:\\t{self.my_recall()}\\nBuilt-in recall score:\\t{recall_score(self.y_true, self.y_pred)}\\n')\n",
    "        \n",
    "        print(f'My f1-score:\\t\\t{self.my_fscore()}\\nBuilt-in f1-score:\\t{f1_score(self.y_true, self.y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e21bc2-ed4e-4a2f-9801-49b5fdb47075",
   "metadata": {},
   "source": [
    "Создадим объект класса для подсчёта метрик:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd7abf0-488d-4f04-af2a-2b9d19d1b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyMetrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22513730-2e60-470e-ba43-1bdda29c7bad",
   "metadata": {},
   "source": [
    "Сравним наши метрики со встроенными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53caa666-c343-4dd8-90fb-52ce172793c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My accuracy score:\t\t0.8\n",
      "Built-in accuracy score:\t0.8\n",
      "\n",
      "My precision score:\t\t0.7142857142857143\n",
      "Built-in precision score:\t0.7142857142857143\n",
      "\n",
      "My recall score:\t0.8333333333333334\n",
      "Built-in recall score:\t0.8333333333333334\n",
      "\n",
      "My f1-score:\t\t0.7692307692307692\n",
      "Built-in f1-score:\t0.7692307692307692\n"
     ]
    }
   ],
   "source": [
    "model.compare()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
